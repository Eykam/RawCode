{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy_of_PS2_2021 (7).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FEz6C4BSRXE"
      },
      "source": [
        "# Problem Set 2: Bayes Classifiers\n",
        "# CMSC422, Fall 2021\n",
        "# Due Sept 30 at 3:30pm \n",
        "\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/thumbnail?id=1R2lmx_HWOkuat3X0jolFYDtcZZhZ0jwm&sz=w1000\" alt=\"iq_hist\" width=\"500px\"/>\n",
        "</center>\n",
        "\n",
        "##Instructions\n",
        "\n",
        "This iPython notebook contains 3 **programming tasks** to be implemented, as well as 4 **written problems**, which have been enumerated for your convenience.\n",
        "\n",
        "**Programming tasks:** In this assignment, you will be asked to implement several variations of Bayes classifiers. As these will be autograded, you must provide your solution to these tasks in the provided code blocks marked with **`# your code here`**. There might be some code already provided for you here, and you may add helper functions in the same code block as necessary.\n",
        "\n",
        "**Written problems:** In addition to programming, you are also given manually-graded analysis questions asking you to interpret results generated by your code (or thereby aid you in doing so).\n",
        "\n",
        "**Submission instructions:** Download this notebook as a `.ipynb` file and submit it to Gradescope. Please note the following before submitting:\n",
        "- Make sure your figures (e.g. plots, tables) are visible when downloading the notebook, otherwise they won't appear on Gradescope.\n",
        "- Make sure your code cells are not throwing exceptions.\n",
        "- Please do not import any packages other than what has already been imported here. You may be penalized for doing so.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LW90REQHRz67"
      },
      "source": [
        "# DO NOT MODIFY THIS BLOCK\n",
        "# imports and some utility function definitions\n",
        "import functools\n",
        "import numpy as np\n",
        "import pandas\n",
        "import math\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "# instantiate RNG\n",
        "prng = np.random.default_rng()\n",
        "\n",
        "# Plot a 2D decision boundary for a binary classifier implemented by cfunc\n",
        "# Arguments:\n",
        "#   cfunc - function implementing the classifier (must take as input a stack of 2-d features, and output a stack of binary classes)\n",
        "#   feat_range - tuple of tuples, describing the range over the 2-d feature space to visualize\n",
        "#   colors - colors to label each class as in the plot when fill=True, otherwise the color of the decision boundaries\n",
        "#   fill - when True, this function fills the regions belonging to each class with the colors; otherwise simply draws the decision boundary\n",
        "#   line_label - used to assign a label to the decision boundary (so that this shows up in any legend added to the plot)\n",
        "#   axes - axes object to plot to; if None, simply plots it to the current active axes (will create one if none are available)\n",
        "def boundplot(cfunc, feat_range=((0,1),(0,1)), colors=['r','b'], fill=True, line_label=None, axes=None):\n",
        "  [x0, x1] = np.meshgrid(*[np.linspace(l, h, num=64) for (l,h) in feat_range], indexing='ij', sparse=False)\n",
        "  x = np.stack([x0.ravel(), x1.ravel()], axis=-1)\n",
        "  y = cfunc(x).reshape(x0.shape)\n",
        "  if axes:\n",
        "    if fill:\n",
        "      contour = axes.contourf(x0,x1,y,levels=[0,0.5,1],colors=colors)\n",
        "    else:\n",
        "      contour = axes.contour(x0,x1,y,levels=[0.5],colors=colors,linewidths=1.5)\n",
        "  else:\n",
        "    if fill:\n",
        "      contour = plt.contourf(x0,x1,y,levels=[0,0.5,1],colors=colors)\n",
        "    else:\n",
        "      contour = plt.contour(x0,x1,y,levels=[0.5],colors=colors,linewidths=1.5)\n",
        "    axes = plt.gca()\n",
        "  axes.set_xlim(feat_range[0])\n",
        "  axes.set_ylim(feat_range[1])\n",
        "  axes.set_aspect('equal', adjustable='box')\n",
        "  if line_label:\n",
        "    contour.collections[0].set_label(line_label)\n",
        "  return contour\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJZFWtPAo_2a"
      },
      "source": [
        "#0. Numpy Review\n",
        "\n",
        "As use of [numpy](https://numpy.org/) for the programming tasks is highly advised, this ungraded section provides a quick overview of some numpy basics. For a comprehensive look at the library, see the [documentation](https://numpy.org/doc/stable/contents.html#numpy-docs-mainpage) and [reference manual](https://numpy.org/doc/stable/reference/index.html).\n",
        "\n",
        "Note that many of the popular machine learning libraries for Python today (e.g. [Tensorflow](https://www.tensorflow.org/), [PyTorch](https://pytorch.org/)) implement datatypes with interfaces similar to those of numpy. Hence, it will help to have an understanding of numpy if you plan on using such libraries.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bk8DTtySGf_e"
      },
      "source": [
        "## Numpy Arrays\n",
        "\n",
        "At the heart of numpy is the $\\verb|ndarray|$ object, a container used to represent fixed-size, homogenous, multi-dimensional arrays. The library allows for a variety of mathematical and other operations to be executed on these array objects with high performance and (often) relatively simple code.\n",
        "\n",
        "For example, suppose we had two 2D arrays $A$ and $B$ of the same dimensions. We would like to compute their element-wise sum: i.e. we want to find an array $C$ of the same dimensions, such that $C_{ij} = A_{ij} + B_{ij}$, where the subscript $_{ij}$ denotes the element at row $i$ and column $j$ of the array.\n",
        "\n",
        "To do this with Python built-ins, one might write the following code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyOEp9GNNdub",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31282028-89ac-4181-c9ed-95d39d9e4f98"
      },
      "source": [
        "A = [[1,2], [3,4], [5,6]]\n",
        "B = [[7,8], [9,10], [11, 12]]\n",
        "C = []\n",
        "for i in range(len(A)):\n",
        "  c_i = []\n",
        "  for j in range(len(A[i])):\n",
        "    c_i.append(A[i][j] + B[i][j])\n",
        "  C.append(c_i)\n",
        "C"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[8, 10], [12, 14], [16, 18]]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XB3yzjBTeH5"
      },
      "source": [
        "Or, alternatively, the following one-liner using list comprehension:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCqHJs42TbuI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a56766ad-78b6-4acd-a63f-e80ad5a29302"
      },
      "source": [
        "[[a_i[0] + b_i[0], a_i[1] + b_i[1]] for a_i, b_i in zip(A,B)]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[8, 10], [12, 14], [16, 18]]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T48WXh4KNeC2"
      },
      "source": [
        "Using numpy, we can compute the same thing with a very simple line of code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3A4GDFwNeoV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3946dc3-fc31-4426-fcff-03a66e2c7426"
      },
      "source": [
        "# convert A, B to ndarrays\n",
        "A_np = np.array(A)\n",
        "B_np = np.array(B)\n",
        "# element-wise addition one-liner; converted back to list for consistent output\n",
        "(A_np + B_np).tolist()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[8, 10], [12, 14], [16, 18]]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPCHw_rBQkLH"
      },
      "source": [
        "The best part is, the numpy code above will run much faster than Python built-ins when $A$ and $B$ contain many elements:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnKLfq7eRi8G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72d507b8-386d-46cb-ca09-d53d4e54cc7a"
      },
      "source": [
        "import timeit\n",
        "py_time = timeit.timeit(\"[[a_i[0] + b_i[0], a_i[1] + b_i[1]] for a_i, b_i in zip(A,B)]\",\n",
        "              setup=\"A = [[i,i+1] for i in range(0,1000,2)]; B = [[i,i+1] for i in range(1000,2000,2)]\",\n",
        "              number=1000)\n",
        "np_time = timeit.timeit(\"A_np + B_np\",\n",
        "              setup=\"import numpy as np; A_np = np.arange(0,1000).reshape((-1,2)); B_np = np.arange(1000,2000).reshape((-1,2))\",\n",
        "              number=1000)\n",
        "print(\"python lists:\\n\\t{:f}\".format(py_time))\n",
        "print(\"numpy arrays:\\n\\t{:f}\".format(np_time))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python lists:\n",
            "\t0.145800\n",
            "numpy arrays:\n",
            "\t0.001017\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNigTraZVYd2"
      },
      "source": [
        "The benchmark above should show that even on arrays with 1000 elements, the numpy summation is already over an order of magnitude faster."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJzDwaOYeuWB"
      },
      "source": [
        "## Quickstart Tutorial\n",
        "\n",
        "The [quickstart page](\n",
        "https://numpy.org/doc/stable/user/quickstart.html) from the official numpy documentation covers everything needed to complete the programing tasks in this assignment. Thus, if you are unfamiliar with numpy, it is highly recommended that you read this page before completing the programming tasks below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_ws0C_MS7Yy"
      },
      "source": [
        "# 1. Naive Bayes Classifier\n",
        "\n",
        "The goal of this section is to implement a Naive Bayes classifier using the *maximum a posteriori* (MAP) decision rule. That is, given an input $\\mathbf{x}$ consisting of $m$ features $(x_0, x_1, \\dots, x_{m-1})$ and a set of $k$ output classes $C = \\{c_0, c_1, \\dots, c_{k-1}\\}$, assign $\\mathbf{x}$ to the class $\\hat{y} \\in C$ satisfying:\n",
        "\n",
        "$$\n",
        "\\hat{y} = \\mathop{\\mathrm{argmax}}_{c_j \\in C} \\,\\left(P(c_j)\\prod_{i=0}^{m-1}P(x_i\\mid{}c_j)\\right) \\tag{1},\n",
        "$$\n",
        "\n",
        "where $P(c_j)$ denotes the probability that a data sample belongs to class $c_j$, and $P(x_i\\mid{}c_j)$ denotes the probability that a data sample, whose class is $c_j$, has  $x_i$ as its $i$-th feature value. See Section 9.3 of [CIML](http://ciml.info/dl/v0_99/ciml-v0_99-ch09.pdf) for the derivation of the above equation, which is equivalent to Equation 9.18 there.\n",
        "\n",
        "For the sake of simplicity, in this exercise we will only consider binary classification problems, where $k = 2$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "px1Nc_hwjfjS"
      },
      "source": [
        "## 1.1 Categorical Naive Bayes\n",
        "\n",
        "When the data is categorical, one way of estimating the probabilites in Equation $(1)$ from observed (training) data is to assume that each of the features belonging to a given class follow some generalized Bernoulli distribution. To keep things simple, in this exercise we will only consider binary features (i.e. each feature can only fall into one of two categories, represented as $0$ or $1$).\n",
        "\n",
        "Thus, to approximate $P(x_i\\mid{}c_j)$, let's hypothesize that for all data points with class $j$, the values of their $i$-th feature follow a Bernoulli distribution with parameter $p_{ij}$ (which determines the probability that the feature takes on the value of $1$). Hence, we have:\n",
        "\n",
        "$$\n",
        "P(x_i\\mid{}c_j) = \\begin{cases} 1 - p_{ij} &\\text{if } x_i = 0, \\\\\n",
        "p_{ij} &\\text{otherwise}.\\end{cases}\n",
        "$$\n",
        "\n",
        "To determine a suitable value of $p_{ij}$ to use, we can use [maximum-likelihood estimation (MLE)](https://mathworld.wolfram.com/MaximumLikelihood.html) based on the training data. For our case, the MLE estimate is simply:\n",
        "\n",
        "$$\n",
        "p_{ij} = \\frac{count(x_i, c_j)}{count(c_j)},\n",
        "$$\n",
        "\n",
        "where $count(c_j)$ is the number of data samples with class $j$, and $count(x_i, c_j)$ is the number of data samples with class $j$ whose $i$-th feature is $x_i$.\n",
        "\n",
        "Similarly, to approximate $P(c_j)$, we can also suppose that the classes themselves follow a Bernoulli distribution. MLE yields:\n",
        "\n",
        "$$\n",
        "P(c_j) = \\frac{count(c_j)}{N},\n",
        "$$\n",
        "\n",
        "where $N$ is the total number of training samples, and $count(c_j)$ is as above.\n",
        "\n",
        "It should be mentioned that there is an issue with using the above estimates in practice: some of the probabilities end up becoming 0 if there are no training examples that fall under the corresponding class and feature combinations. This may lead to cases where the expression being maximized in Equation $(1)$ becomes 0 for one, more, or perhaps all hypothesis classes. [Laplace smoothing](https://en.wikipedia.org/wiki/Additive_smoothing) is a popluar way to remedy this, but such methods are out of this assignment's scope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSKAHt0PlQcb"
      },
      "source": [
        "### Programming Task 1 (30 points):\n",
        "\n",
        "Implement a binary Naive Bayes classifier for categorical data by filling out the class below, using the assumptions above. In particular, the class must implement the $\\verb|__init__|$ function and the $\\verb|predict|$ function, as detailed below. Do *not* use any smoothing.\n",
        "\n",
        "The $\\verb|__init__|$ function is called when the classifier is initialized, and will be supplied the training data we want the classifier to use.\n",
        ">Arguments:\n",
        ">\n",
        ">* $\\verb|features|$: a ndarray of shape $(N,M)$, containing the $M$ feature variables for each of the $N$ training examples. This array has values of $0$ and $1$, corresponding to the two possible categories each feature can take.\n",
        ">\n",
        ">* $\\verb|labels|$: a ndarray of shape $(N,)$, containing the corresponding labels for each of the $N$ training examples. This array has values of $0$ and $1$, corresponding to the two possible output classes of a binary classification problem.\n",
        "\n",
        "Once initialized, the $\\verb|predict|$ function can then be called to classify new data.\n",
        "\n",
        ">Arguments:\n",
        ">\n",
        ">* $\\verb|x|$: a ndarray of shape $(B,M)$, containing the $M$ feature values for each of the $B \\geq 1$ new data points. Contains values of $0$ and $1$, corresponding to the two possible categories each feature can take.\n",
        ">\n",
        ">Returns:\n",
        ">\n",
        ">* A ndarray of shape $(B,)$, containing the predicted output classes of each provided data point. This array should have values of $0$ and $1$, corresponding to the two output classes of a binary classification problem.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-BEqk7NoBu1"
      },
      "source": [
        "class CategoricalNaiveBayes():\n",
        "  def __init__(self, features, labels):\n",
        "    x1_c0 = 0\n",
        "    x2_c0 = 0\n",
        "    x3_c0 = 0\n",
        "    x1_c1 = 0\n",
        "    x2_c1 = 0\n",
        "    x3_c1 = 0\n",
        "    tot_c0 = 0 \n",
        "    tot_c1 = 0\n",
        "\n",
        "    pred_table = np.column_stack((features,labels))\n",
        "    for x in range(len(pred_table)):\n",
        "      if (pred_table[x][3] == 0):\n",
        "        x1_c0 += pred_table[x][0]\n",
        "        x2_c0 += pred_table[x][1]\n",
        "        x3_c0 += pred_table[x][2]\n",
        "        tot_c0 += 1 \n",
        "\n",
        "      else:\n",
        "        x1_c1 += pred_table[x][0]\n",
        "        x2_c1 += pred_table[x][1]\n",
        "        x3_c1 += pred_table[x][2]\n",
        "        tot_c1 += 1\n",
        "  \n",
        "    self.count_table = [[x1_c0/100,x2_c0/100,x3_c0/100],[x1_c1/100,x2_c1/100,x3_c1/100]]\n",
        "    self.tot_c0 = .5\n",
        "    self.tot_c1 = .5\n",
        "\n",
        "\n",
        "  def predict(self, x):\n",
        "    prediction = []\n",
        "    for i in range(len(x)):\n",
        "      prob_0 = 0\n",
        "      prob_1 = 0\n",
        "      x1 = 0\n",
        "      x2 = 0\n",
        "      x3 = 0\n",
        "\n",
        "      #calculating prob_0\n",
        "      if (x[i][0] == 0):\n",
        "        x1 = 1-self.count_table[0][0]\n",
        "      else:\n",
        "        x1 = self.count_table[0][0]\n",
        "\n",
        "      if (x[i][1] == 0):\n",
        "        x2 = 1-self.count_table[0][1]\n",
        "      else:\n",
        "        x2 = self.count_table[0][1]\n",
        "\n",
        "      if (x[i][2] == 0):\n",
        "        x3 = 1-self.count_table[0][2]\n",
        "      else:\n",
        "        x3 = self.count_table[0][2]\n",
        "\n",
        "\n",
        "\n",
        "      prob_0 = x1*x2*x3*self.tot_c0\n",
        "\n",
        "      #Calculating prob_1  \n",
        "      if (x[i][0] == 0):\n",
        "        x1 = 1-self.count_table[1][0]\n",
        "      else:\n",
        "        x1 = self.count_table[1][0]\n",
        "\n",
        "      if (x[i][1] == 0):\n",
        "        x2 = 1-self.count_table[1][1]\n",
        "      else:\n",
        "        x2 = self.count_table[1][1]\n",
        "\n",
        "      if (x[i][2] == 0):\n",
        "        x3 = 1-self.count_table[1][2]\n",
        "      else:\n",
        "        x3 = self.count_table[1][2]\n",
        "\n",
        "      prob_1 = x1*x2*x3*self.tot_c1\n",
        "\n",
        "      if (prob_0 > prob_1):\n",
        "        prediction.append(0)\n",
        "      else:\n",
        "        prediction.append(1)\n",
        "    \n",
        "    return np.array(prediction)\n",
        "      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKcjeIvK1hMN"
      },
      "source": [
        "Let's check our implementation against some example data. Consider a dataset with 2 classes $(c_0, c_1)$ and 3 binary features $(x_0, x_1, x_2)$. Suppose we have 100 samples from each class. The data samples can be summarized as the table:\n",
        "\n",
        "$$\n",
        "\\begin{array}{|c|c|c|c|}\n",
        "\\hline\n",
        " & x_0 = 1 & x_1 = 1 & x_2 = 1 \\\\\n",
        "\\hline\n",
        "c_0 & 50 & 20 & 80 \\\\\n",
        "c_1 & 60 & 70 & 40 \\\\\n",
        "\\hline\n",
        "\\end{array}\n",
        "$$\n",
        "\n",
        "For example, the entry in the \"$c_0$\" row and \"$x_0 = 1$\" column denotes that there are 50 samples with class $c_0$ whose $x_0$ feature has value $1$. Since we have 100 samples for each class, there are thus 50 samples with class $c_0$ whose $x_0$ feature has value $0$.\n",
        "\n",
        "Before running our implementation, let's first find the expected behavior of Naive Bayes on such a dataset by hand. We can then use this to verify the implementation's behavior."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsSNkibMS9BP"
      },
      "source": [
        "### Written Problem 1 (8 points):\n",
        "\n",
        "For each of the 8 possible points in the feature space, derive the expected behavior of a Naive Bayes classifier on the dataset described above. As per Equation $(1)$, this involves finding the class $c_j$ that maximizes the following quantity:\n",
        "\n",
        "$$\n",
        "score(c_j) = P(c_j)\\prod_{i=0}^{2}P(x_i\\mid{}c_j) = P(c_j)P(x_0\\mid{}c_j)P(x_1\\mid{}c_j)P(x_2\\mid{}c_j).\n",
        "$$\n",
        "\n",
        "To help you get started, the following blank table has been provided below, which can simply be filled out."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qH6j0k_-UuQu"
      },
      "source": [
        "*your answer here*\n",
        "\n",
        "$$\n",
        "\\begin{array}{|c|c|c|c|}\n",
        "\\hline\n",
        "(x_0,x_1,x_2) & score(c_0) & score(c_1) & \\text{output class} \\\\\n",
        "\\hline\n",
        "(0,0,0)       & .04        &     .036   &         C0          \\\\\n",
        "(0,0,1)       &  .16       &     .024   &         C0          \\\\\n",
        "(0,1,0)       &    .01     &     .084   &         C1          \\\\\n",
        "(0,1,1)       &   .04      &     .056   &         C1          \\\\\n",
        "(1,0,0)       &   .04      &     .054   &         C1          \\\\\n",
        "(1,0,1)       &   .16      &     .024   &         C0          \\\\\n",
        "(1,1,0)       &   .01      &     .126   &         C1          \\\\\n",
        "(1,1,1)       &   .04       &    .084   &         C1          \\\\\n",
        "\\hline\n",
        "\\end{array}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dVaDDxDaXzr"
      },
      "source": [
        "Now, let's compare the expected behavior we've derived with the result of our \n",
        "\n",
        "*   List item\n",
        "*   List item\n",
        "\n",
        "implementation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KbBBOEbad5q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "outputId": "8bfc0394-baeb-4baa-a89d-8f725e59b003"
      },
      "source": [
        "# DO NOT MODIFY THIS BLOCK\n",
        "seeded = np.random.default_rng(314159) # seeded to ensure deterministic behavior\n",
        "# generate a dataset conforming to the description above\n",
        "thresh = [[50, 60], [20, 70], [80, 40]] # the desired sample counts\n",
        "# for each class, uniformly pick the desired number of samples, assign these features as 1, the rest as 0\n",
        "x = [[], []]\n",
        "for thresh_i in thresh:\n",
        "  x[0].append((seeded.permutation(100) < thresh_i[0]).astype(np.int))\n",
        "  x[1].append((seeded.permutation(100) < thresh_i[1]).astype(np.int))\n",
        "# aggregate the two classes, and shuffle the data\n",
        "x[0] = np.stack(x[0], axis=-1)\n",
        "x[1] = np.stack(x[1], axis=-1)\n",
        "x = np.concatenate(x, axis=0)\n",
        "idx = prng.permutation(x.shape[0])\n",
        "x = x[idx,...]\n",
        "y = (np.arange(x.shape[0]) >= 100)[idx]\n",
        "# set up model\n",
        "model = CategoricalNaiveBayes(x, y)\n",
        "# evaluate model for every point in our input space, and arrange it into a table\n",
        "result_table = np.empty((4,8), dtype=np.int)\n",
        "for num in range(0,8):\n",
        "  x_query = np.array([(num >> 2) & 1, (num >> 1) & 1, num & 1])\n",
        "  result_table[0:3,num] = x_query\n",
        "  result_table[3,num] = model.predict(np.expand_dims(x_query, axis=0))[0]\n",
        "result = pandas.DataFrame({'x0' : result_table[0,:], 'x1' : result_table[1,:], 'x2' : result_table[2,:], 'prediction' : result_table[3,:]})\n",
        "display(HTML(result.to_html(index=False)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>x0</th>\n",
              "      <th>x1</th>\n",
              "      <th>x2</th>\n",
              "      <th>prediction</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aue9G39XjRbv"
      },
      "source": [
        "Verify that the result is as expected."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpmS3q8ZjW5U"
      },
      "source": [
        "### Written Problem 2 (6 points):\n",
        "\n",
        "Suppose the features in the data above are indeed independent, and that the Bernoulli distributions we used to approximate $P(x_i\\mid{}c_j)$ and $P(c_j)$ fit the true data distribution exactly. What is the classification error rate for our Naive Bayes classifier?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ki5LrZjynrxs"
      },
      "source": [
        "*your answer here*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-VdMtLLjgxM"
      },
      "source": [
        "## 1.2 Gaussian Naive Bayes\n",
        "\n",
        "When dealing with continuous, numerical data, we might instead estimate the relevant probabilities in Equation $(1)$ by assuming that each of the features belonging to a given class follow some normal (Gaussian) distribution. Thus, we can then use the corresponding probability density function to approximate the probability $P(x_i\\mid{}c_j)$ for feature $i$ and class $j$, as follows:\n",
        "\n",
        "$$\n",
        "P(x_i\\mid{}c_j) = \\frac{1}{\\sigma_{ij}\\sqrt{2\\pi}}e^{-\\frac{1}{2}\\left(\\frac{x_i - \\mu_{ij}}{\\sigma_{ij}}\\right)^2},\n",
        "$$\n",
        "\n",
        "where $\\mu_{ij}, \\sigma_{ij}$ are the mean and standard deviation of feature $i$ conditioned class $j$. The [MLE estimates for a Gaussian distribution](https://mathworld.wolfram.com/MaximumLikelihood.html) are simply the sample mean and deviation of training samples with class $j$.\n",
        "\n",
        "Since the classes themselves remain categorical, we can still estimate $P(c_j)$ with a Bernoulli distribution, as was done previously.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrnfHKH7AEb_"
      },
      "source": [
        "### Programming Task 2 (30 points):\n",
        "\n",
        "Implement a binary Naive Bayes classifier for continuous data by filling out the class below. As in Problem 1, the class must implement the $\\verb|__init__|$ function and the $\\verb|predict|$ function, as detailed below.\n",
        "\n",
        "The $\\verb|__init__|$ function is called when the classifier is initialized, and will be supplied the training data we want the classifier to use.\n",
        ">Arguments:\n",
        ">\n",
        ">* $\\verb|features|$: a ndarray of shape $(N,M)$, containing the $M$ feature variables for each of the $N$ training examples.\n",
        ">\n",
        ">* $\\verb|labels|$: a ndarray of shape $(N,)$, containing the corresponding labels for each of the $N$ training examples. This array has values of $0$ and $1$, corresponding to the two possible output classes of a binary classification problem.\n",
        "\n",
        "Once initialized, the $\\verb|predict|$ function can then be called to classify new data.\n",
        "\n",
        ">Arguments:\n",
        ">\n",
        ">* $\\verb|x|$: a ndarray of shape $(B,M)$, containing the feature values of $B \\geq 1$ new data points.\n",
        ">\n",
        ">Returns:\n",
        ">\n",
        ">* A ndarray of shape $(B,)$, containing the predicted output classes of each provided data point. This array should have values of $0$ and $1$, corresponding to the two output classes of a binary classification problem.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZe6ClhF7AGZ"
      },
      "source": [
        "class GaussianNaiveBayes():\n",
        "  def __init__(self, features, labels):\n",
        "\n",
        "    self.labels = labels\n",
        "\n",
        "    cols = {\"labels\":labels}\n",
        "    for x in range(len(features[0])):\n",
        "      cols[\"x\"+str(x)] = features[:,x]\n",
        "\n",
        "    df = pandas.DataFrame(cols)\n",
        "    avgs = df.groupby(['labels']).mean()\n",
        "    std = df.groupby(['labels']).std()\n",
        "\n",
        "    \n",
        "    self.avgs = avgs\n",
        "    self.std = std\n",
        "    self.counts = [len(labels) - sum(labels), sum(labels)]\n",
        "\n",
        "    # print(self.avgs)\n",
        "    # print(self.std)\n",
        "    # print(self.avgs.iloc[0,0])\n",
        "    # print(self.std.iloc[1,1])\n",
        "\n",
        "\n",
        "    \n",
        "  def norm_Pdf(self, value, mean, std):\n",
        "    var = float(std)**2\n",
        "    denom = (2*math.pi*var)**.5\n",
        "    num = math.exp(-(float(value)-float(mean))**2/(2*var))\n",
        "    return num/denom\n",
        "      \n",
        "  def predict(self, x):\n",
        "    predictions = []\n",
        "\n",
        "    for row in range(len(x)):\n",
        "      pred_0 = self.counts[0]/sum(self.counts)\n",
        "      pred_1 = self.counts[1]/sum(self.counts)\n",
        "      \n",
        "      for col in range(len(x[row])):\n",
        "        pred_0 *= GaussianNaiveBayes.norm_Pdf(self, x[row][col], self.avgs.iloc[col,0], self.std.iloc[col,0])\n",
        "        pred_1 *= GaussianNaiveBayes.norm_Pdf(self, x[row][col], self.avgs.iloc[col,1], self.std.iloc[col,1])\n",
        "      \n",
        "      if (pred_0 - pred_1 > 0):\n",
        "        predictions.append(0)\n",
        "  \n",
        "      else:\n",
        "        predictions.append(1)\n",
        "  \n",
        "    return np.array(predictions)      \n"
      ],
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ju9cR774Hbsq"
      },
      "source": [
        "Let's test the implementation on some example data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ap_JO1nSHixG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "outputId": "46b9e195-210f-4756-9ada-f70aed901be5"
      },
      "source": [
        "# DO NOT MODIFY THIS BLOCK\n",
        "# draw samples from two classes that are each gaussian-distributed\n",
        "x_0 = prng.multivariate_normal([0.3,0.2], np.square(np.array([[0.15,0],[0,0.05]])), size=(512,))\n",
        "x_1 = prng.multivariate_normal([0.8,0.5], np.square(np.array([[0.6,0],[0,0.2]])), size=(512,))\n",
        "# display the samples\n",
        "ax = plt.axes()\n",
        "ax.scatter(x_0[...,0], x_0[...,1], s=10, c='b')\n",
        "ax.scatter(x_1[...,0], x_1[...,1], s=10, c='r')\n",
        "# create our data array: aggregate the classes' samples and shuffle\n",
        "x = np.concatenate([x_0, x_1], axis=0)\n",
        "y = (np.arange(x.shape[0]) < 512)\n",
        "idx = prng.permutation(x.shape[0])\n",
        "x = x[idx,...]\n",
        "y = y[idx]\n",
        "# set up model and plot its decision boundary\n",
        "model = GaussianNaiveBayes(x ,y)\n",
        "boundplot(lambda x : model.predict(x), colors=['k'], fill=False, line_label='decision boundary', axes=ax)\n",
        "ax.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left')\n",
        "plt.tight_layout()"
      ],
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              x0        x1\n",
            "labels                    \n",
            "False   0.758412  0.512993\n",
            "True    0.298496  0.199048\n",
            "              x0        x1\n",
            "labels                    \n",
            "False   0.579948  0.198504\n",
            "True    0.145409  0.048716\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7axUQqVVGZx"
      },
      "source": [
        "As a sanity check for yourself, make sure that the decision boundary shown above is sensible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3xfmWHtVcQY"
      },
      "source": [
        "Now let's see how well our Gaussian-prior classifier works on data that does not meet our prior assumptions. For simplicity, let's define our data such that our two classes are uniformly distributed in mutually exclusive areas in the feature space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMIXdBN4VmkP"
      },
      "source": [
        "# DO NOT MODIFY THIS BLOCK\n",
        "# four datasets in which the ground-truth class for each point in our feature space\n",
        "# (which is simply the unit square) is determined by the labelfuncs below\n",
        "x = prng.uniform(0.0,1.0,size=(4096,2))\n",
        "labelfuncs = [lambda x : (x[...,0] < x[...,1]),\n",
        "              lambda x : (np.abs(x[...,0] - 0.5) < np.abs(x[...,1] - 0.5)),\n",
        "              lambda x : ((x[...,0] < 0.5) * (x[...,1] < 0.5) + (x[...,0] > 0.5) * (x[...,1] > 0.5)),\n",
        "              lambda x : ((np.sqrt(np.sum(np.square(x - 0.5), axis=-1)) > 0.15) * (np.sqrt(np.sum(np.square(x - 0.5), axis=-1)) < 0.4))]\n",
        "\n",
        "for labelfunc in labelfuncs:\n",
        "  # apply the ground-truth labels for this dataset\n",
        "  y = labelfunc(x)\n",
        "  # plot and compare the ground-truth distribution and model output\n",
        "  _, axarr = plt.subplots(1,2)\n",
        "  # ground-truth\n",
        "  boundplot(labelfunc, axes=axarr[0])\n",
        "  axarr[0].set_title('ground truth')\n",
        "  # our model\n",
        "  model = GaussianNaiveBayes(x, y)\n",
        "  boundplot(lambda x : model.predict(x), fill=True, axes=axarr[1])\n",
        "  axarr[1].set_title('classifier output')\n",
        "  plt.tight_layout()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4fUdmJRbial"
      },
      "source": [
        "### Written Problem 3 (8 points):\n",
        "\n",
        "How well does the classifier perform in these cases? Why do you think the classifier behaves the way it does in each of the four cases?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQ8hIQlqbmI8"
      },
      "source": [
        "*your answer here*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wY-Lbjy6l9Kj"
      },
      "source": [
        "# 2. Bayes Optimal Classifier\n",
        "\n",
        "Recall that the Naive Bayes formulation shown in Equation $(1)$ is a simplification for that of the Bayes optimal classifier:\n",
        "\n",
        "$$\n",
        "\\hat{y} = \\mathop{\\mathrm{argmax}}_{c_j \\in C} \\,\\left(P(c_j)P(\\mathbf{x}\\mid{}c_j)\\right) \\tag{2},\n",
        "$$\n",
        "\n",
        "where $\\mathbf{x} = (x_0, x_1, \\dots, x_{m-1})$ are the features of an input sample, $C = \\{c_0, c_1, \\dots, c_{k-1}\\}$ are the output classes, and $P(\\mathbf{x}\\mid{}c_j)$ is the joint probability density for all features, conditioned on class $c_j$ and evaluated at $\\mathbf{x}$.\n",
        "\n",
        "Of course, we typically don't know $P(\\mathbf{x}\\mid{}c_j)$ in practice, and estimating it is often intractable for the complex datasets found in the wild. Nonetheless, it can be useful as a benchmark to compare a practical classifier with the Bayes optimal on a synthetic dataset (for which $P(\\mathbf{x}\\mid{}c_j)$ is known)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JkCux1I1meD"
      },
      "source": [
        "### Programming Task 3 (10 points):\n",
        "\n",
        "Implement a binary Bayes Optimal classifier for continuous data by filling out the class below. Conforming to the previous classifier implementations, the class must implement the $\\verb|__init__|$ function and the $\\verb|predict|$ function, as detailed below.\n",
        "\n",
        "The $\\verb|__init__|$ function is called when the classifier is initialized, and will be supplied the probability density functions we want the classifier to use.\n",
        ">Arguments:\n",
        ">\n",
        ">* $\\verb|pdf_0|$, $\\verb|pdf_1|$: functions that, when called, return the probability density of the corresponding class evaluated at a query point. Each function takes as input a ndarray of shape $(B,M)$, consisting of $B \\geq 1$ query points in the $M$-dimensional feature space, and outputs a ndarray of shape $(B,)$ containing the corresponding probability densities.\n",
        ">\n",
        ">* $\\verb|prob_0|$: the class prior probability for the first class (i.e. $P(c_0)$). Note that this also implicitly determines the prior probability for the second class, as they must sum to $1$. Will always be in range $[0,1]$.\n",
        "\n",
        "Once initialized, the $\\verb|predict|$ function can then be called to classify new data.\n",
        "\n",
        ">Arguments:\n",
        ">\n",
        ">* $\\verb|x|$: a ndarray of shape $(B,M)$, containing the feature values of $B \\geq 1$ new data points.\n",
        ">\n",
        ">Returns:\n",
        ">\n",
        ">* A ndarray of shape $(B,)$, containing the predicted output classes of each provided data point. This array should have values of $0$ and $1$, corresponding to the two output classes of a binary classification problem.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYUNgy8XIMNs"
      },
      "source": [
        "class BayesOptimal():\n",
        "  def __init__(self, pdf_0, pdf_1, prob_0):\n",
        "    # your code here\n",
        "    pass\n",
        "\n",
        "  def predict(self, x):\n",
        "    # your code here\n",
        "    pass\n",
        "    \n",
        "  # feel free to add any helper functions here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEQUpM1fG1Fs"
      },
      "source": [
        "Let's test our Bayes optimal classifier on some correlated, Gaussian-distributed data, and compare it to our Gaussian Naive Bayes classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-07oWLYatmk"
      },
      "source": [
        "# DO NOT MODIFY THIS BLOCK\n",
        "import scipy.stats\n",
        "from scipy.stats import multivariate_normal as mvn\n",
        "# the parameters for our example data distribution (mean and covariance)\n",
        "mean_0 = np.array([0.4,0.3])\n",
        "mean_1 = np.array([0.8,0.2])\n",
        "cov_0 = np.array([[200,150],[150,140]])/10000\n",
        "cov_1 = np.array([[40,-100],[-100,500]])/10000\n",
        "# draw samples from the distributions\n",
        "x_0 = prng.multivariate_normal(mean_0, cov_0, size=(512,))\n",
        "x_1 = prng.multivariate_normal(mean_1, cov_1, size=(512,))\n",
        "# plot drawn samples\n",
        "ax = plt.axes()\n",
        "ax.scatter(x_0[...,0], x_0[...,1], s=10, c='b')\n",
        "ax.scatter(x_1[...,0], x_1[...,1], s=10, c='r')\n",
        "# aggregate classes, shuffle\n",
        "x = np.concatenate([x_0, x_1], axis=0)\n",
        "y = (np.arange(x.shape[0]) < 512)\n",
        "idx = prng.permutation(x.shape[0])\n",
        "x = x[idx,...]\n",
        "y = y[idx]\n",
        "# set up models and plot their decision boundaries\n",
        "model_naive = GaussianNaiveBayes(x, y)\n",
        "model_opt = BayesOptimal(functools.partial(mvn.pdf, mean=mean_0, cov=cov_0), functools.partial(mvn.pdf, mean=mean_1, cov=cov_1), 0.5)\n",
        "boundplot(lambda x : model_naive.predict(x), colors=['m'], fill=False, line_label='decision boundary (naive)', axes=ax)\n",
        "boundplot(lambda x : model_opt.predict(x), colors=['c'], fill=False, line_label='decision boundary (optimal)', axes=ax)\n",
        "ax.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left')\n",
        "plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOvhyKI7qKUv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbOi4uMQHnsg"
      },
      "source": [
        "### Written Problem 4 (8 points):\n",
        "\n",
        "Which classifier performs better here? Why? Under what circumstances, if any, would the two classifiers perform equally well? Which one do you think will work better in most of the real-life scenarios and why ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4ktER8bJ2Nh"
      },
      "source": [
        "*your answer here*"
      ]
    }
  ]
}